\documentclass[11pt]{article}

\usepackage[sets]{cryptocode}
\newcommand{\prob}[1]{\ensuremath{\operatorname{Pr}\left( #1 \right)}}
\newcommand{\condprob}[2]{\prob{#1\, \middle|\, #2}}

\usepackage{booktabs}
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\usepackage{csquotes}

\newcommand\mdusk{\ensuremath{\mathtt{dusk}}}
\newcommand\mdawn{\ensuremath{\mathtt{dawn}}}

\title{Imperfect Secrecy and information updating}
\author{Jeffrey Goldberg}

\begin{document}
\maketitle

This document emerged out of questions surrounding the notion of a one-time-pad and why perfect secrecy is impossible if the key is shorter than the plaintext.
Instead of pointing people to the relevant theorems, I thought I could take the opportunity to push people toward Bayesian thinking.
In doing so I constructed a toy example. I then decided to actually work through the math of my toy example, thinking it would be a straightforward illustration of how cool it is to use Bayesian reasoning.
Unfortunately I found that working through the example using Bayes Rule is not at all straightforward to someone who isn't already familiar with it.
Instead of just giving up, I continued to dig myself in deeper into trying to explain things. What you see before you is the result so far.

\section{A no(ta)tional introduction}

In what follows, I am going to talk a lot about conditional probability.
Let's consider two related propositions:

\begin{description}
    \item[$\neg S$] The sun is not shining on some particular spot on Earth
    \item[$R$] It is raining on that particular spot.  
\end{description}

What can we surmise (given our understanding of weather and the world) about $\neg S$ if $R$ happens to be true? While we know that it is possible for rain to fall where the sun is shining, in a large majority of the cases if it is raining somewhere the sun is not shining on that spot. So we can reasonably surmise that given that it is raining on the spot the sun probably isn't shining there.

We write the probability of $\neg S$ as \prob{\neg S} and the probability of $R$ as \prob{R}. We write the conditional probability that the sun is shining given that it is raining as \condprob{\neg S}{R}.
That should be read as
\enquote{the probability of $\neg S$ given $R$}
or \enquote{given that $R$ is true, the probability that $\neg S$ is true}.

What is extremely important to understand is that the probability of $\neg S$ given $R$ is not the same as the probability of $R$ given $\neg S$.
If the sun is not shining on a particular spot, what are the chances that it is raining on that spot? Well, it could be night. It could be cloud with no rain (it might even be snowing).
The chances that raining given that the sun is not shining is certainly less than 50\%. 
Contrast that with the typical whether patterns that allowed us to surmise that if $R$ holds then it is likely that $\neg S$ does too.

We may not know the exact probabilities, but we do know that \condprob{\neg S}{R} is greater than \condprob{S}{\neg S}.

\subsection{Bayes, hypotheses, and experiments}

One of the greatest things ever is Bayes Rule or Bayes Theorem that (given some other information) allows us to compute \condprob{X}{Y} from \condprob{Y}{X}.
There is some fascinating psychology on how humans systematically get such probability assessments wrong, but that is for another day. Instead, I am going to focus on the situation that will arise with our toy example.

Suppose you are a scientist with a hypothesis, $H$, and you run an experiment that gives you result $E$ to test the hypothesis. A huge amount of 20th century statistics and data analysis techniques gave us ever more powerful ways to estimate the probability of getting your particular experimental result given your hypothesis. That is, we became very good at computing \condprob{E}{H}.

But that really isn't what we are interested in.
Knowing how likely it is to get the data given a hypothesis isn't really that interesting.
What we want to know is \condprob{H}{E}. That is, given this evidence, $E$, how likely is hypothesis $H$. I don't care how likely my evidence is given my hypothesis; what I want to know is how likely my hypothesis is to be true given the evidence. Bayesian analysis allows us to go after that far more interesting question.

\subsection{Before and after}

There is a reason why Bayesian analysis wasn't as popular in the 20th century as you might think.\footnote{%
    If you guessed that it is because Bayesian analysis is a new invention, you are mistaken. It as old as probability theory itself. 
    It is foundational in Information Theory,
    and it was central to the cryptanalytic advances led by Alan Turing at Bletchley Park, but it was left by the wayside in most statistics.
}
That reason is because you need to have some estimate of the probability that your hypothesis is true prior to your experiment.
The Bayesian magic of getting to \condprob{H}{E} from \condprob{E}{H} requires that you have \emph{some} idea of $H$ to begin with.
Another way of putting this is that Bayesian techniques are about \emph{updating} prior probability estimates given new evidence.
You have to already have something you are updating.

Anyway, the probability that you start with for $H$ is often called the \enquote{prior probability} and the updated estimate is called the \enquote{posterior probability}. You will see these terms below.

\subsection{Semantic security}

I digressed a bit, but the point is that we need to be thinking in terms of evidence updating our prior state of knowledge.
When we consider our hypotheses as being about the plaintext of a message and our experiment as examining the content of some corresponding ciphertext we start to sneak up on the definition of semantic security.

I am not going to define semantic security formally. There are plenty of places you can for for that. The essence of the notion is that examining the bits of some ciphertext gives you no more information about about the plaintext than you already had. This can be stated in terms of conditional probability.
We are saying that having the bits of the ciphertext doesn't change your knowledge about the plaintext.

It turns out that when this is formally defined it is equivalent to
(a suitable definition of) indistinguishability under a
chosen plaintext attacks.\footnote{%
    The proof is left as an exercise to the reader. Well in Katz \& Lindell half of the proof is left as an exercise to the reader.
}
This is why distinguishers are such a big a deal.
If you can show that an algorithm exists that can act as a distinguisher under a CPA, you know that you don't have semantic security.




\section{The toy}

Because we are talking about perfect secrecy and contrasts with it, we have no computational bounds on the algorithms run. We are not limiting ourselves to efficient algorithms.

To help illustrate why a key must be as long as the plaintext in order to achieve perfect secrecy, let's consider a case in which a key is 1-bit smaller than then the plaintext.

Suppose that we\footnote{%
    We are the attacker here. It just became much easier to write this thing when talking about what \enquote{we} learn and \enquote{our} states of knowledge.
}
have captured a ciphertext of a 14 byte (112 bit) message.
Suppose also that we very strongly suspects that the plaintext message is one of either “\texttt{attack at dawn}” or “\texttt{attack at dusk}”.
We will simply refer to these as the \mdusk\ message or the \mdawn\ message.

If our algorithm runs through all $2^{112}$ possible pads it will give us all $2^{112}$ possible plaintexts.
The two suspected messages, \mdawn\ and \mdusk, will be among them.
The output of this algorithm will not allow us to update any our probability assessments. We learn nothing new.

But now suppose the plaintext is encrypted with a 111-bit key, and the best attack on the cipher is to brute force the key space.
With our unbounded computing power we can run through all possible 111-bit keys and produce $2^{111}$ candidate plaintexts.
And because $2^{111}$ half of $2^{112}$
we will have ruled out half of all possible plaintexts,
and ruling out half of the possible plaintexts is learning something new.
We may not know which of the $2^{111}$ candidate plaintexts is the right one, but we know that if \mdawn\ is not among them, then \mdawn\ cannot be the original message.

For reasons which may or may not become clear later, I am going to call the this run of trying all $2^{111}$ keys the experiment. 
With respect to \mdawn\ and \mdusk, there are four distinct experimental outcomes:

\begin{enumerate}
    \item\label{en:dawn} \mdawn, but not \mdusk, is among the outputs.
        We will call this experimental outcome $E_a$.
    \item\label{en:dusk} \mdusk, but not \mdawn, is among the outputs.
        We will call this experimental outcome $E_u$.
    \item\label{en:neither} Neither \mdusk\ nor \mdawn\ are among the outputs.
    We will call this experimental outcome $E_n$.
    \item\label{en:both} Both \mdawn\ and \mdusk\ are among the $2^{111}$ outputs.
    We will call this experimental outcome $E_b$.
\end{enumerate}

In experimental outcome $E_a$ the attacker no knows that the probability of the message being \texttt{attack at dusk} is zero.
The attacker has definitely learned something new.
Likewise, in $E_u$ the attacker no knows that the probability of the message being \texttt{attack at dawn} is zero.
The attacker has also learned something new.

In the experimental outcome that neither is in the output ($E_n$) the attacker learns
that neither could have been the plaintext and so has to treat the probability of both suspected messages to zero. The attacker may not like it, but this is the case where they learn the most.

The attacker learns something, though not much, with outcome $E_b$.
It nudges the probabilities of both \mdusk\ and \mdawn\ a bit closer to 0.5.

\section{Computation}

We have three hypotheses, \mdawn, \mdusk, and neither.
We have four possible types of events given our $2^{111}$ possible types of keys.

Because we are talking about how we are updating probabilities, Bayes' Rule is what we will use.

For any particular experimental outcome, $E$ we can update the probability of each hypothesis using Bayes Rule.

\begin{equation}\label{eq:bayesH}
    \condprob{H_i}{E} = \frac{\condprob{E}{H_i}\prob{H_i}}{\prob{E}}
\end{equation}
where
\begin{equation}\label{eq:ProbE}
    \prob{E} = \sum \condprob{E}{H_i}\prob{H_i}
\end{equation}

We will have $E$ range over our four different experiment outcomes.

Equation~(\ref{eq:bayesH} may not be useful to you, but it is certainly useful for me as I am writing this.

Let's give ourselves some concrete numbers to work with.
Before conducting the experiment the attacker assigns these probabilities:

\begin{align*}
    \prob{M = \mdawn} &= 0.45 && \text{(we call this $H_a$)} \\
    \prob{M = \mdusk} &= 0.45  && \text{(we call this $H_u$)} \\
    H_n & = 1 - (H_a + H_u) 
\end{align*}

\subsection{\(E = E_a\)}

Let's start with experimental outcome $E_a$. That is we ran through all 111-bit keys and found the \mdawn\ message among the $2^{111}$ outputs but did not find the \mdusk\ message.

We want to update our probability assignments conditioned on getting $E_a$. That is, our goal in this section is to compute \condprob{H_a}{E_a}, \condprob{H_u}{E_a}, \condprob{H_n}{E_a}.

We don't need to go through the Bayesian math for \condprob{H_u}{E_a}.
There is simply no way that $H_u$ could be true if we don't see the \mdusk\ message among our outputs. So we know that $\condprob{H_u}{E_a} = 0$.

We also know that \condprob{H_n}{E_a} is just what is left over from the other two hypotheses, so $\condprob{H_n}{E_a} = 1 - \condprob{H_a}{E_a}$.
So we will only really need to compute \condprob{H_a}{E_a} using Bayes Rule.


We start by computing all of the conditional probabilities from needed for the right hand side of equation~(\ref{eq:bayesH}). This means computing the likelihood of getting $E_a$ given each of the hypotheses. 

We need to first calculate the probabilities of getting the given experimental outcome $E_a$ given each of the hypotheses.
We can immediately conclude $\condprob{E_a}{H_u} = 0$.
If \mdusk\ is the answer, there is no way that we could get an experimental result without an \mdusk\ message.

This leaves us with $H_a$ and $H_n$.
We only need to figure out one of them, because the other will just 1 minus whichever we compute.
Because we have gone through half of the $2^{112}$ possible messages,
we can assume\footnote{%
    We can make this assumption because we are already pretending that the best attack on this cipher is to go through all of the keys. If the output within that $2^{111}$ candidate plaintext space were distinguishable from random, then this assumption would not be safe.
}
anything that isn't the actual plaintext has a 0.5 probability\footnote{%
    It is actually a teeny-tiny bit smaller, as one spot in the list of outputs
    is taken up by the real plaintext. So it is $(2^{111}-1)/2^{112}$ chance.
    But that differs from 0.5 by a negligible amount.)
}
of showing up in our experimental output.
So both \condprob{E_a}{H_a} and \condprob{E_a}{H_n} are 0.5.

Now we can compute \prob{E_a} using equation~\ref{eq:ProbE}.

\begin{equation}
    \begin{split}
        \prob{E_a}  &= \sum \condprob{E_a}{H_i}\prob{H_i} \\
                    &= \condprob{E_a}{H_a}\prob{H_a}
                        + \condprob{E_a}{H_u}\prob{H_u}
                        + \condprob{E_a}{H_n}\prob{H_n} \\
                    &= 0.5 \cdot 0.45 + 0 \cdot 0.45 + 0.5 \cdot 0.1 \\
                    & 0.275
    \end{split}
\end{equation}

And now we have all of the values we need to plug into equation~\ref{eq:bayesH}

\begin{equation}
    \begin{split}
        \condprob{H_a}{E_a}
            &= \frac{\condprob{E_a}{H_a}\prob{H_a}}{\prob{E_a}} \\
            &= \frac{0.5 \cdot 0.45}{0.275} \\
            &\approx 0.82
    \end{split}
\end{equation}

So we can now update the probabilities of for each hypothesis

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior from $E_a$} \\
        \midrule
        $H_a$                       & 0.45      & 0.82 \\
        $H_u$                       & 0.45      & 0 \\
        $H_n$                       & 0.10      & 0.18 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_a\) updates our prior probabilities}
    \end{center}
\end{table}

\subsection{\(E = E_u\)} 

Because we started out with the same prior probabilities $H_a$ and $H_u$
this is exactly the same computation as for the $E_a$ just swapping $\mathcal{X}_a$ and $\mathcal{X}_u$. 

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior from $E_u$} \\
        \midrule
        $H_a$                       & 0.45      & 0 \\
        $H_u$                       & 0.45      & 0.82 \\
        $H_n$                       & 0.10      & 0.18 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_u\) updates our prior probabilities}
    \end{center}
\end{table}

\subsection{\(E = E_b\)}

Do we learn something new when both the \mdawn\ and \mdusk\ messages show up in our experiment? You'd be right to think that the result isn't very informative; it isn't. But there is a small bit that we learn.
The intuition is that when we exclude half of the possible $2^112$ plaintexts we still see \mdawn\ and \mdusk\ in the running, and so this lowers the chance that neither is the true plaintext. Now we do the math to see how much probability assessments are changed by such a result. (It isn't much, but it is something.)

Once again, we need to work out all of the parts for the right hand side of equation~\ref{eq:bayesH}. That means finding the probability of getting $E_b$ given each of our three hypotheses. Once again, we only really need to work through calculating one of \condprob{E_b}{H_a} or \condprob{E_b}{H_u} as the others we can compute easily once we have one of those.

If $H_a$ is true, the only two possible experimental results we could get are  $E_b$ and $E_a$. Given that we are generating half of all possible plaintexts, \mdusk\ should show about half the time given that it is not the true plaintext. So $\condprob{E_b}{H_a} = 0.5$.
Similarly, if $H_u$ correct then \mdawn\ has a 50\% chance of showing up in our experiment, so \condprob{E_b}{H_u}.
If $H_n$ is correct there is independently a 0.5 chance of \mdawn\ showing up, and there is a 0.5 chance of \mdusk\ showing up;
So $\condprob{E_b}{H_n} = 0.25$

\begin{equation}
    \begin{split}
        \prob{E_b}  &= \sum \condprob{E_b}{H_i}\prob{H_i} \\
                    &= \condprob{E_b}{H_a}\prob{H_a}
                        + \condprob{E_b}{H_u}\prob{H_u}
                        + \condprob{E_b}{H_n}\prob{H_n} \\
                    &= 0.5 \cdot 0.45 + 0.5 \cdot 0.45 + 0.25 \cdot 0.1 \\
                    & 0.475
    \end{split}
\end{equation}

\begin{equation}
    \begin{split}
        \condprob{H_a}{E_b}
            &= \frac{\condprob{E_b}{H_a}\prob{H_a}}{\prob{E_b}} \\
            &= \frac{0.5 \cdot 0.45}{0.475} \\
            &\approx 0.474
    \end{split}
\end{equation}

\condprob{H_u}{E_b} will be the same, and so $\condprob{H_n}{E_b} = 1 -2\condprob{H_a}{E_b} \approx 0.053$.

While the results, shown in table~\ref{tab:postEb}, don't really make a substantial difference to our prior views of $H_a$ and $H_u$,
it does cut our $H_n$ prior nearly in half.

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior from $E_b$} \\
        \midrule
        $H_a$                       & 0.45      & 0.474 \\
        $H_u$                       & 0.45      & 0.474\\
        $H_n$                       & 0.10      & 0.053 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_b\) updates our prior probabilities}
    \label{tab:postEb}
    \end{center}
\end{table}





\subsection{\(E = E_n\)}

Let's now work through the case where the experiment spit out neither \mdawn\ nor \mdusk. We really don't need to do the whole Bayesian thing here as we can see that that this means that we now know exactly which of our three hypotheses is true. 


We compute all of the conditional probabilities from needed for the right hand side of equation~(\ref{eq:bayesH}).


The probability of getting $E_n$ if $H_a$ is true is zero. That is
$\condprob{E_n}{H_a} = 0$.
There is no way we could get experimental result $H_n$ if the plaintext message is the \mdawn\ message.
Similarly 
$\condprob{E_n}{H_u} = 0$.
And because $H_n = 1 - (H_a + H_u)$, we know that $\condprob{E_n}{H_n}$ = 1.

We don't need to compute \prob{E_n} for this case as long as we know it isn't zero.\footnote{It isn't zero}.

And that gives us \condprob{H_a}{E_n} (the probability that $H_a$ is true given the experimental result $E_n$)

\begin{align*}
    \condprob{H_a}{E_n} &= \frac{\condprob{E_n}{H_a}}{\prob{E_n}} \\
                        &= \frac{0 \cdot 0.45}{\prob{E_n}} \\
                        & 0 
\end{align*}
The same holds for $\condprob{H_u}{E_n}$. And since $\condprob{H_n}{E_n}$ is just what is left over from the sum of  $\condprob{H_a}{E_n}$ and $\condprob{H_u}{E_n}$, we know that the probability that the plaintext is neither the \mdusk\ message nor the \mdawn\ message given $E_n$ is 1.




\end{document}




