\documentclass[11pt]{article}

\usepackage[sets]{cryptocode}
\newcommand{\prob}[1]{\ensuremath{\operatorname{Pr}\left( #1 \right)}}
\newcommand{\condprob}[2]{\prob{#1\, \middle|\, #2}}

\usepackage{amsthm}
\newtheorem{definition}{Definition}

\usepackage{booktabs}
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\usepackage{csquotes}

\newcommand\mdusk{\ensuremath{\mathtt{dusk}}}
\newcommand\mdawn{\ensuremath{\mathtt{dawn}}}

\title{Imperfect Secrecy and information updating}
\author{Jeffrey Goldberg}

\begin{document}
\maketitle

This document emerged out of questions surrounding the notion of a one-time-pad and why perfect secrecy is impossible if the key is shorter than the plaintext.
Instead of pointing people to the relevant theorems, I thought I could take the opportunity to push people toward Bayesian thinking.
In doing so I constructed a toy example. I then decided to actually work through the math of my toy example, thinking it would be a straightforward illustration of how cool it is to use Bayesian reasoning.
Unfortunately I found that working through the example using Bayes Rule is not at all straightforward to someone who isn't already familiar with it.
Instead of just giving up, I continued to dig myself in deeper into trying to explain things. What you see before you is the result so far.

\section{A no(ta)tional introduction}

In what follows, I am going to talk a lot about conditional probability.
Let's consider two related propositions:

\begin{description}
    \item[$\neg S$] The sun is not shining on some particular spot on Earth
    \item[$R$] It is raining on that particular spot.  
\end{description}

What can we surmise (given our understanding of weather and the world) about $\neg S$ if $R$ happens to be true? While we know that it is possible for rain to fall where the sun is shining, in a large majority of the cases if it is raining somewhere the sun is not shining on that spot. So we can reasonably surmise that given that it is raining on the spot the sun probably isn't shining there.

We write the probability of $\neg S$ as \prob{\neg S} and the probability of $R$ as \prob{R}. We write the conditional probability that the sun is shining given that it is raining as \condprob{\neg S}{R}.
That should be read as
\enquote{the probability of $\neg S$ given $R$}
or \enquote{given that $R$ is true, the probability that $\neg S$ is true}.

What is extremely important to understand is that the probability of $\neg S$ given $R$ is not the same as the probability of $R$ given $\neg S$.
If the sun is not shining on a particular spot, what are the chances that it is raining on that spot? Well, it could be night. It could be cloud with no rain (it might even be snowing).
The chances that raining given that the sun is not shining is certainly less than 50\%. 
Contrast that with the typical whether patterns that allowed us to surmise that if $R$ holds then it is likely that $\neg S$ does too.

We may not know the exact probabilities, but we do know that \condprob{\neg S}{R} is greater than \condprob{S}{\neg S}.

\subsection{Bayes, hypotheses, and experiments}

One of the greatest things ever is Bayes Rule or Bayes Theorem that (given some other information) allows us to compute \condprob{X}{Y} from \condprob{Y}{X}.
There is some fascinating psychology on how humans systematically get such probability assessments wrong, but that is for another day. Instead, I am going to focus on the situation that will arise with our toy example.

Suppose you are a scientist with a hypothesis, $H$, and you run an experiment that gives you result $E$ to test the hypothesis. A huge amount of 20th century statistics and data analysis techniques gave us ever more powerful ways to estimate the probability of getting your particular experimental result given your hypothesis. That is, we became very good at computing \condprob{E}{H}.

But that really isn't what we are interested in.
Knowing how likely it is to get the data given a hypothesis isn't really that interesting.
What we want to know is \condprob{H}{E}. That is, given this evidence, $E$, how likely is hypothesis $H$. I don't care how likely my evidence is given my hypothesis; what I want to know is how likely my hypothesis is to be true given the evidence. Bayesian analysis allows us to go after that far more interesting question.

\subsection{Before and after}

There is a reason why Bayesian analysis wasn't as popular in the 20th century as you might think.\footnote{%
    If you guessed that it is because Bayesian analysis is a new invention, you are mistaken. It as old as probability theory itself. 
    It is foundational in Information Theory,
    and it was central to the cryptanalytic advances led by Alan Turing at Bletchley Park, but it was left by the wayside in most statistics.
}
That reason is because you need to have some estimate of the probability that your hypothesis is true prior to your experiment.
The Bayesian magic of getting to \condprob{H}{E} from \condprob{E}{H} requires that you have \emph{some} idea of $H$ to begin with.
Another way of putting this is that Bayesian techniques are about \emph{updating} prior probability estimates given new evidence.
You have to already have something you are updating.

Anyway, the probability that you start with for $H$ is often called the \enquote{prior probability} and the updated estimate is called the \enquote{posterior probability}. You will see these terms below.

\subsection{Semantic security}

I digressed a bit, but the point is that we need to be thinking in terms of evidence updating our prior state of knowledge.
When we consider our hypotheses as being about the plaintext of a message and our experiment as examining the content of some corresponding ciphertext we start to sneak up on the definition of semantic security.

I am not going to define semantic security formally. There are plenty of places you can for for that. The essence of the notion is that examining the bits of some ciphertext gives you no more information about about the plaintext than you already had. This can be stated in terms of conditional probability.
We are saying that having the bits of the ciphertext doesn't change your knowledge about the plaintext.

It turns out that when this is formally defined it is equivalent to
(a suitable definition of) indistinguishability under a
chosen plaintext attacks.\footnote{%
    The proof is left as an exercise to the reader. Well in Katz \& Lindell half of the proof is left as an exercise to the reader.
}
This is why distinguishers are such a big a deal.
If you can show that an algorithm exists that can act as a distinguisher under a CPA, you know that you don't have semantic security.

\section{The toy}

To help illustrate why a key must be as long as the plaintext in order to achieve perfect secrecy, let's consider a case in which a key is 1-bit smaller than then the plaintext.

Suppose that we\footnote{%
    We are the attacker here. It just became much easier to write this thing when talking about what \enquote{we} learn and \enquote{our} states of knowledge.
}
have captured a ciphertext of a 14 byte (112 bit) message.
Suppose also that we very strongly suspects that the plaintext message is one of either “\texttt{attack at dawn}” or “\texttt{attack at dusk}”.
We will simply refer to these as the \mdusk\ message or the \mdawn\ message.

We can characterize our prior suspicions as three competing hypotheses:

\begin{description}
    \item[$H_a$] The plaintext is “\texttt{attack at dawn}”.,
    \item[$H_u$] The plaintext is “\texttt{attack at dusk}”.
    \item[$H_n$] The plaintext is neither of those.\footnote{%
        We don't really need to define this as a separate hypothesis. It is simply $\neg H_a \land \neg H_u$, but it turned out easier to write this up if at times I pretend it is a separate hypothesis.}
\end{description}
Exactly one of those hypotheses must be true.

If our algorithm runs through all $2^{112}$ possible pads,\footnote{%
    Because we are talking about perfect secrecy,
    we have no computational bounds on the algorithms run.
    We are not limiting ourselves to efficient algorithms.}
it will give us all $2^{112}$ possible plaintexts.
The two suspected messages, \mdawn\ and \mdusk, will be among them.
The output of this algorithm will not allow us to update our hypotheses.
We learn nothing about the plaintext that we didn't already know even though we have some strong ideas about what the plaintext should look like.

But now suppose the plaintext is encrypted with a 111-bit key, and the best attack on the cipher is to brute force the key space.
With our unbounded computing power we can run through all possible 111-bit keys and produce $2^{111}$ candidate plaintexts.
And because $2^{111}$ half of $2^{112}$
we will have ruled out half of all possible plaintexts,
and ruling out half of the possible plaintexts is learning something new.
We may not know which of the $2^{111}$ candidate plaintexts is the right one, but we know that if \mdawn\ is not among them, then \mdawn\ cannot be the original message.

I am going to call the this run of trying all $2^{111}$ keys the experiment. 
With respect to \mdawn\ and \mdusk, there are four distinct experimental outcomes:

\begin{enumerate}
    \item\label{en:dawn} \mdawn, but not \mdusk, is among the outputs.
        We will call this experimental outcome $E_a$.
    \item\label{en:dusk} \mdusk, but not \mdawn, is among the outputs.
        We will call this experimental outcome $E_u$.
    \item\label{en:neither} Neither \mdusk\ nor \mdawn\ are among the outputs.
    We will call this experimental outcome $E_n$.
    \item\label{en:both} Both \mdawn\ and \mdusk\ are among the $2^{111}$ outputs.
    We will call this experimental outcome $E_b$.
\end{enumerate}
The result of the experiment must be exactly one of those outcomes.

If we get experimental outcome $E_a$
we learn that $H_u$ must be false.
There is no way that the plaintext could have been the \mdusk\ message,
and so we have definitely learned something new.
Our experiment and the evidence it provides allows us to revise at least one of our hypotheses.

Similarly, if we get $E_u$ we know that $H_a$ must be false.
The evidence from our experiment allows us to revise at least one of our prior hypotheses.

If our experiment yields $E_n$,
we learn that neither $H_a$ nor $H_u$ can be true and that $H_n$ must be true.
This may not be to our liking, as it shows that we were really wrong in our
original ideas about what the plaintext would be, but it also is the most informative outcome. It dramatically changes our probability judgements of the hypotheses.

The attacker learns something, though not much, with outcome $E_b$.
It nudges the probabilities of both \mdusk\ and \mdawn\ a bit closer to 0.5.

What we see is that at least some (in fact all) of the possible outcomes of working through all of the possible keys with a key size that is just 1 bit smaller then the plaintext allows us to our knowledge of the plaintext, even if just in terms of probabilities.
Examining the contents of the ciphertext allows us to learn something new about the plaintext.

Of course our algorithm required exponential time in the key size.
But we are talking about \emph{perfect} secrecy. so we are given that power.

\section{Computation}

Let's give ourselves some concrete numbers to work with.
Suppose that before examining the contents to the ciphertext, we 
suppose that there is a 45\% chance that the plaintext is the \mdawn\ message,
and that there is also a 45\% chance that the plaintext is the \mdusk\ message.
This leaves us with a 10\% chance that it is neither.
We spell out these prior probabilities in equation~\ref{eq:priors}.

\begin{equation}\label{eq:priors}
\begin{split}
    \prob{H_a} &= 0.45\\
    \prob{H_u} &= 0.45\\
    \prob{H_n} &= 0.10
\end{split}
\end{equation}

We want to know how each of the four possible experimental outcomes adjusts those probabilities.

\subsection{\(E = E_n\)}

Let's first consider $E_n$, where neither the \mdawn\ message nor the \mdusk\ message appear among our candidate plaintexts.
It should clear that we simply rule out $H_a$ and $H_u$,
and we recognize that $H_n$ must be true.

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior from $E_n$} \\
        \multicolumn{1}{c}{$H_i$}
        & \multicolumn{1}{c}{\prob{H_i}} 
        & \multicolumn{1}{c}{\condprob{H_i}{E_n}} \\
        \midrule
        $H_a$                       & 0.45      & 0 \\
        $H_u$                       & 0.45      & 0 \\
        $H_n$                       & 0.10      & 1 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_n\) updates our prior probabilities}
    \label{tab:postEn}
    \end{center}
\end{table}

Although what we see in table~\ref{tab:postEb} is obvious, let me use this simple case to introduce ways of talking about parts of this.

When we say that it is obvious that we can rule out $H_a$ we are saying so because there is no way that $H_a$ can be true if we get $E_n$.
That is $\condprob{H_a}{E_b} = 0$.
In the same way, we are saying that noting that $\condprob{H_u}{E_b} = 0$.
And because exactly one of our three hypotheses must be true, we know that 
$\condprob{H_n}{E_b} = 1$.

I am saying all of that multiple times because in what follows
we will have to actually calculate those conditional probabilities using Bayes' Rule.

\subsection{Bayes' Rule}

For the other three cases we are going to need to use Bayes' Rule (also known as Bayes' Theorem).
As I said in the introduction, Bayes' Rule gives us a way to get from
\condprob{E}{H} to \condprob{H}{E}.
The formulation of it that you see in definition~\ref{def:bayes} is more complicated than I would like, but it is what is needed when we are considering multiple hypotheses.

I am not going to attempt to explain it here, but it really isn't that hard to understand what it is saying and why it works. It's just that that would be a whole other lecture. 

\begin{definition}[Bayes' Theorem]\label{def:bayes}
\begin{equation}\label{eq:bayesH}
    \condprob{H_i}{E} = \frac{\condprob{E}{H_i}\prob{H_i}}{\prob{E}}
\end{equation}
where
\begin{equation}\label{eq:ProbE}
    \prob{E} = \sum \condprob{E}{H_i}\prob{H_i}
\end{equation}
\end{definition}

One thing that I would like to draw your attention to is that on the right hand side of the formulae, we only have the prior probabilities, \prob{H} and conditional probabilities that are all probabilities of evidence given hypotheses, \condprob{E}{H}.
In each of the cases we look at, we will need to figure out that conditional probabilities.  

\subsection{\(E = E_a\)}

Let's start with experimental outcome $E_a$. That is we ran through all 111-bit keys and found the \mdawn\ message among the $2^{111}$ outputs but did not find the \mdusk\ message.

Our goal in this section is to compute how we adjust our probability assessments given $E_a$.
In other words we want to compute \condprob{H_a}{E_a}, \condprob{H_u}{E_a},
and \condprob{H_n}{E_a}. Now is a good time to read those aloud as
\enquote{the probability that $H_a$ is true, given the experimental result $E_a$},
\enquote{the probability that $H_u$ is true, given the experimental result $E_a$},
and  \enquote{the probability that $H_n$ is true, given the experimental result $E_a$}. It is important to understand what we are after.


We need to do some work to get all of the pieces on the right hand side of Bayes' Rule. We already have our prior probabilities for the hypotheses,
as listed in equation~\ref{eq:priors}. So what we will need to figure out are
\condprob{E_a}{H_a}, \condprob{E_a}{H_u}, and \condprob{E_a}{H_n}.

We can quickly see that $\condprob{E_a}{H_u} = 0$.
There is simply no way that $H_u$ could be true if we don't see the \mdusk\ message among our outputs. So we know that $\condprob{H_u}{E_a} = 0$.

This leaves us with \condprob{E_a}{H_a} and \condprob{E_a}{H_u}
We only need to figure out one of them, because the other will just 1 minus whichever we compute. We will work on \condprob{E_a}{H_a}.

Because we have gone through half of the $2^{112}$ possible messages,
we can assume
anything that isn't the actual plaintext has a 0.5 probability\footnote{%
    We can make this assumption because we are already pretending that the best attack on this cipher is to go through all of the keys.
    If the output within that $2^{111}$ candidate plaintext space were distinguishable from random, then this assumption would not be safe.

    It is actually a teeny-tiny bit smaller, as one spot in the list of outputs
    is taken up by the real plaintext. So it is $(2^{111}-1)/2^{112}$ chance.
    But that differs from 0.5 by a negligible amount.)
}
of showing up in our experimental output.
What this tells us that even if $H_n$ is true, we could still get $E_a$ half the time.
So both \condprob{E_a}{H_a} and \condprob{E_a}{H_n} are 0.5.

We now have 
\begin{itemize}
    \item $\condprob{E_a}{H_a} = 0.5$
    \item $\condprob{E_a}{H_u} = 0$
    \item $\condprob{E_a}{H_n} = 0.5$
\end{itemize}

Now we can compute \prob{E_a} using equation~\ref{eq:ProbE}.

\begin{equation}
    \openup 1\jot
    \begin{split}
        \prob{E_a}  &= \sum \condprob{E_a}{H_i}\prob{H_i} \\
                    &= \condprob{E_a}{H_a}\prob{H_a}
                        + \condprob{E_a}{H_u}\prob{H_u}
                        + \condprob{E_a}{H_n}\prob{H_n} \\
                    &= 0.5 \cdot 0.45 + 0 \cdot 0.45 + 0.5 \cdot 0.1 \\
                    &= 0.275
    \end{split}
\end{equation}

And now we have all of the values we need to plug into equation~\ref{eq:bayesH}

\begin{equation}
    \openup 1\jot
    \begin{split}
        \condprob{H_a}{E_a}
            &= \frac{\condprob{E_a}{H_a}\prob{H_a}}{\prob{E_a}} \\
            &= \frac{0.5 \cdot 0.45}{0.275} \\
            &\approx 0.82
    \end{split}
\end{equation}

So we can now update the probabilities of for each hypothesis.
Remember that $\condprob{H_n}{E_a} = 1 - (\condprob{H_a}{E_a} + \condprob{H_u}{E_a})$.

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior given $E_a$} \\
        \multicolumn{1}{c}{$H_i$}
        & \multicolumn{1}{c}{\prob{H_i}} 
        & \multicolumn{1}{c}{\condprob{H_i}{E_a}} \\
        \midrule
        $H_a$                       & 0.45      & 0.82 \\
        $H_u$                       & 0.45      & 0 \\
        $H_n$                       & 0.10      & 0.18 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_a\) updates our prior probabilities}
    \label{tab:postEa}
    \end{center}
\end{table}

\subsection{\(E = E_u\)} 

Because we started out with the same prior probabilities $H_a$ and $H_u$
this is exactly the same computation as for the $E_a$ just swapping $\mathcal{X}_a$ and $\mathcal{X}_u$. 

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior given $E_u$} \\
        \multicolumn{1}{c}{$H_i$}
        & \multicolumn{1}{c}{\prob{H_i}} 
        & \multicolumn{1}{c}{\condprob{H_i}{E_u}} \\
        \midrule
        $H_a$                       & 0.45      & 0 \\
        $H_u$                       & 0.45      & 0.82 \\
        $H_n$                       & 0.10      & 0.18 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_u\) updates our prior probabilities}
    \label{tab:postEu}
    \end{center}
\end{table}

\subsection{\(E = E_b\)}

Do we learn something new when both the \mdawn\ and \mdusk\ messages show up in our experiment? You'd be right to think that the result isn't very informative; it isn't.
But there is a small bit that we learn.
The intuition is that when we exclude half of the possible $2^112$ plaintexts we still see \mdawn\ and \mdusk\ in the running, and so this lowers the chance that neither is the true plaintext. Now we do the math to see how much probability assessments are changed by such a result. (It isn't much, but it is something.)

Once again, we need to work out all of the parts for the right hand side of equation~\ref{eq:bayesH}. That means finding the probability of getting $E_b$ given each of our three hypotheses. Once again, we only really need to work through calculating one of \condprob{E_b}{H_a} or \condprob{E_b}{H_u} as the others we can compute easily once we have one of those.

If $H_a$ is true, the only two possible experimental results we could get are  $E_b$ and $E_a$. Given that we are generating half of all possible plaintexts, \mdusk\ should show up about half the time given that it is not the true plaintext. So $\condprob{E_b}{H_a} = 0.5$.
Similarly, if $H_u$ correct then \mdawn\ has a 50\% chance of showing up in our experiment, so \condprob{E_b}{H_u}.
If $H_n$ is correct there is independently a 0.5 chance of \mdawn\ showing up, and there is a 0.5 chance of \mdusk\ showing up;
So $\condprob{E_b}{H_n} = 0.25$

\begin{equation}
    \openup 1\jot
    \begin{split}
        \prob{E_b}  &= \sum \condprob{E_b}{H_i}\prob{H_i} \\
                    &= \condprob{E_b}{H_a}\prob{H_a}
                        + \condprob{E_b}{H_u}\prob{H_u}
                        + \condprob{E_b}{H_n}\prob{H_n} \\
                    &= 0.5 \cdot 0.45 + 0.5 \cdot 0.45 + 0.25 \cdot 0.1 \\
                    &= 0.475
    \end{split}
\end{equation}

\begin{equation}
    \openup 2\jot
    \begin{split}
        \condprob{H_a}{E_b}
            &= \frac{\condprob{E_b}{H_a}\prob{H_a}}{\prob{E_b}} \\
            &= \frac{0.5 \cdot 0.45}{0.475} \\
            &\approx 0.474
    \end{split}
\end{equation}

\condprob{H_u}{E_b} will be the same, and so $\condprob{H_n}{E_b} = 1 -2\condprob{H_a}{E_b} \approx 0.053$.

While the results, shown in table~\ref{tab:postEb}, don't really make a substantial difference to our prior views of $H_a$ and $H_u$,
it does cut our $H_n$ prior nearly in half.

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior from $E_b$} \\
        \multicolumn{1}{c}{$H_i$}
        & \multicolumn{1}{c}{\prob{H_i}} 
        & \multicolumn{1}{c}{\condprob{H_i}{E_b}} \\
        \midrule
        $H_a$                       & 0.45      & 0.474 \\
        $H_u$                       & 0.45      & 0.474\\
        $H_n$                       & 0.10      & 0.053 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_b\) updates our prior probabilities}
    \label{tab:postEb}
    \end{center}
\end{table}


\end{document}




