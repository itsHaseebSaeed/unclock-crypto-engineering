\documentclass{article}

\usepackage[probability,sets]{cryptocode}
\usepackage{booktabs}
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}

\newcommand\mdusk{\ensuremath{\mathtt{dusk}}}
\newcommand\mdawn{\ensuremath{\mathtt{dawn}}}

\begin{document}
To help illustrate why a key must be as long as the plaintext in order to achieve perfect secrecy, let's consider a case in which a key is 1-bit smaller than then the plaintext.

Suppose the attacker has captured a ciphertext of a 14 byte (112 bit) message.
Suppose also that the attacker very strongly suspects that the message is one of either “\texttt{attack at dawn}” or ”\texttt{attack at dusk}”.
We will simply refer to these as the \mdusk\ message or the \mdawn\ message.

If an algorithm runs through all $2^{112}$ possible pads it will give the attacker $2^{112}$ possible plaintexts.
The two suspected messages, \mdawn\ and \mdusk, will be among them.
The output of this algorithm will not allow the attacker to update any of their probability assessments. They learn nothing new.

But now suppose the plaintext is encrypted with a 111 bit key, and the best attack on the cipher is to brute force the key space.
An attacker with unlimited computing power could run through all possible 111-bit keys and produce $2^{111}$ candidate plaintexts.
$2{111}$ half of $2^{112}$.
So the attacker has now ruled out half of all possible plaintexts,
and ruling out half of the possible plaintexts is learning something new.

For reasons which may or may not become clear later, I am going to call the this run of trying all $2^{111}$ keys the experiment. 
With respect to \mdawn\ and \mdusk, there are four distinct experimental outcomes:

\begin{enumerate}
    \item\label{en:dawn} \mdawn, but not \mdusk, is among the outputs.
        We will call this experimental outcome $E_a$.
    \item\label{en:dusk} \mdusk, but not \mdawn, is among the outputs.
        We will call this experimental outcome $E_u$.
    \item\label{en:neither} Neither \mdusk\ nor \mdawn\ are among the outputs.
    We will call this experimental outcome $E_n$.
    \item\label{en:both} Both \mdawn\ and \mdusk\ are among the $2^{111}$ outputs.
    We will call this experimental outcome $E_b$.
\end{enumerate}

In experimental outcome $E_a$ the attacker no knows that the probability of the message being \texttt{attack at dusk} is zero.
The attacker has definitely learned something new.
Likewise, in $E_u$ the attacker no knows that the probability of the message being \texttt{attack at dawn} is zero.
The attacker has also learned something new.

In the experimental outcome that neither is in the output ($E_n$) the attacker learns
that neither could have been the plaintext and so has to treat the probability of both suspected messages to zero. The attacker may not like it, but this is the case where they learn the most.

The attacker learns something, though not much, with outcome $E_b$.
It nudges the probabilities of both \mdusk\ and \mdawn\ a bit closer to 0.5.

\section{Computation}

We have three hypotheses, \mdawn, \mdusk, and neither.
We have four possible types of events given our $2^{111}$ possible types of keys.

Because we are talking about how we are updating probabilities, Bayes' Rule is what we will use.

For any particular experimental outcome, $E$ we can update the probability of each hypothesis using Bayes Rule.

\begin{equation}\label{eq:bayesH}
    \condprob{H_i}{E} = \frac{\condprob{E}{H_i}\prob{H_i}}{\prob{E}}
\end{equation}
where
\begin{equation}\label{eq:ProbE}
    \prob{E} = \sum \condprob{E}{H_i}\prob{H_i}
\end{equation}

We will have $E$ range over our four different experiment outcomes.

Equation~(\ref{eq:bayesH} may not be useful to you, but it is certainly useful for me as I am writing this.

Let's give ourselves some concrete numbers to work with.
Before conducting the experiment the attacker assigns these probabilities:

\begin{align*}
    \prob{M = \mdawn} &= 0.45 && \text{(we call this $H_a$)} \\
    \prob{M = \mdusk} &= 0.45  && \text{(we call this $H_u$)} \\
    H_n & = 1 - (H_a + H_u) 
\end{align*}

\subsection{\(E = E_a\)}

Let's start with experimental outcome $E_a$. That is we ran through all 111-bit keys and found the \mdawn\ message among the $2^{111}$ outputs but did not find the \mdusk\ message.

We want to update our probability assignments conditioned on getting $E_a$. That is, our goal in this section is to compute \condprob{H_a}{E_a}, \condprob{H_u}{E_a}, \condprob{H_n}{E_a}.

We don't need to go through the Bayesian math for \condprob{H_u}{E_a}.
There is simply no way that $H_u$ could be true if we don't see the \mdusk\ message among our outputs. So we know that $\condprob{H_u}{E_a} = 0$.

We also know that \condprob{H_n}{E_a} is just what is left over from the other two hypotheses, so $\condprob{H_n}{E_a} = 1 - \condprob{H_a}{E_a}$.
So we will only really need to compute \condprob{H_a}{E_a} using Bayes Rule.


We start by computing all of the conditional probabilities from needed for the right hand side of equation~(\ref{eq:bayesH}). This means computing the likelihood of getting $E_a$ given each of the hypotheses. 

We need to first calculate the probabilities of getting the given experimental outcome $E_a$ given each of the hypotheses.
We can immediately conclude $\condprob{E_a}{H_u} = 0$.
If \mdusk\ is the answer, there is no way that we could get an experimental result without an \mdusk\ message.

This leaves us with $H_a$ and $H_n$.
We only need to figure out one of them, because the other will just 1 minus whichever we compute.
Because we have gone through half of the $2^{112}$ possible messages,
we can assume\footnote{%
    We can make this assumption because we are already pretending that the best attack on this cipher is to go through all of the keys. If the output within that $2^{111}$ candidate plaintext space were distinguishable from random, then this assumption would not be safe.
}
anything that isn't the actual plaintext has a 0.5 probability\footnote{%
    It is actually a teeny-tiny bit smaller, as one spot in the list of outputs
    is taken up by the real plaintext. So it is $(2^{111}-1)/2^{112}$ chance.
    But that differs from 0.5 by a negligible amount.)
}
of showing up in our experimental output.
So both \condprob{E_a}{H_a} and \condprob{E_a}{H_n} are 0.5.

Now we can compute \prob{E_a} using equation~\ref{eq:ProbE}.

\begin{equation}
    \begin{split}
        \prob{E_a}  &= \sum \condprob{E_a}{H_i}\prob{H_i} \\
                    &= \condprob{E_a}{H_a}\prob{H_a}
                        + \condprob{E_a}{H_u}\prob{H_u}
                        + \condprob{E_a}{H_n}\prob{H_n} \\
                    &= 0.5 \cdot 0.45 + 0 \cdot 0.45 + 0.5 \cdot 0.1 \\
                    & 0.275
    \end{split}
\end{equation}

And now we have all of the values we need to plug into equation~\ref{eq:bayesH}

\begin{equation}
    \begin{split}
        \condprob{H_a}{E_a}
            &= \frac{\condprob{E_a}{H_a}\prob{H_a}}{\prob{E_a}} \\
            &= \frac{0.5 \cdot 0.45}{0.275} \\
            &\approx 0.82
    \end{split}
\end{equation}

So we can now update the probabilities of for each hypothesis

\begin{table}
    \begin{center}
    \begin{tabular}{l..}
        \toprule
        \multicolumn{1}{c}{Hypothesis}
        & \multicolumn{1}{c}{Prior} 
        & \multicolumn{1}{c}{Posterior from $E_a$} \\
        \midrule
        $H_a$                       & 0.45      & 0.82 \\
        $H_u$                       & 0.45      & 0 \\
        $H_n$                       & 0.10      & 0.18 \\
        \bottomrule
    \end{tabular}
    \caption{How a result of \(E_a\) updates our prior probabilities}
    \end{center}
\end{table}

\subsection{\(E = E_n\)}

Let's now work through the case where the experiment spit out neither \mdawn\ nor \mdusk. We really don't need to do the whole Bayesian thing here as we can see that that this means that we now know exactly which of our three hypotheses is true. 


We compute all of the conditional probabilities from needed for the right hand side of equation~(\ref{eq:bayesH}).


The probability of getting $E_n$ if $H_a$ is true is zero. That is
$\condprob{E_n}{H_a} = 0$.
There is no way we could get experimental result $H_n$ if the plaintext message is the \mdawn\ message.
Similarly 
$\condprob{E_n}{H_u} = 0$.
And because $H_n = 1 - (H_a + H_u)$, we know that $\condprob{E_n}{H_n}$ = 1.

We don't need to compute \prob{E_n} for this case as long as we know it isn't zero.\footnote{It isn't zero}.

And that gives us \condprob{H_a}{E_n} (the probability that $H_a$ is true given the experimental result $E_n$)

\begin{align*}
    \condprob{H_a}{E_n} &= \frac{\condprob{E_n}{H_a}}{\prob{E_n}} \\
                        &= \frac{0 \cdot 0.45}{\prob{E_n}} \\
                        & 0 
\end{align*}
The same holds for $\condprob{H_u}{E_n}$. And since $\condprob{H_n}{E_n}$ is just what is left over from the sum of  $\condprob{H_a}{E_n}$ and $\condprob{H_u}{E_n}$, we know that the probability that the plaintext is neither the \mdusk\ message nor the \mdawn\ message given $E_n$ is 1.




\end{document}




